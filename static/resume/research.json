[
  {
    "name": "Transforming Diffusion",
    "kind": "Survey paper",
    "description": "Research on transformers, diffusion models and their combination. In this work we explore the transformer architecture, attempts to reduce its complexity, ways to scale them up and its use for image synthesis. Our exploration covered cutting-edge technologies such as Stable Diffusion, Imagen, eDiff-I, Reformer, Bert, GPT-3 and many others. This research was based on the Transformers United course at Stanford University (CS25).",
    "start": "Sep 2022",
    "end": "Jan 2023",
    "image": "static/diffusion.png",
    "paper": "static/diffusion_paper.pdf",
    "presentation": "static/diffusion_ppt.pdf",
    "source": "https://web.stanford.edu/class/cs25/"
  },
  {
    "name": "Embodied Language Models",
    "kind": "Survey paper",
    "description": "In this research project, we will delve into the world of Embodied Language Models. This topic is approached from a deep multi-task and meta-learning angle. We explain multi-task and meta-learning concepts and delve into their applications in the field of reinforcement learning. This foundation is then used to explain how language models can be used to enhance generalisation in robotics and allow for long-term planning. This research is based on the Deep multi-task & meta learning course at Stanford University (CS330).",
    "start": "Feb 2023",
    "end": "Jun 2023",
    "footnote": "Part 2 of research project based on Stanford courses (6 ECTS credits)",
    "image": "static/palme.png",
    "paper": "static/embodied_paper.pdf",
    "presentation": "static/embodied_ppt.pdf",
    "source": "https://cs330.stanford.edu/"
  },
  {
    "name": "Improving the Forward-Forward Algorithm",
    "kind": "Thesis",
    "description": "As machine learning models become increasingly complex, they also become more difficult to train and optimize. Local learning, which focuses on updating subsets of a model's parameters at a time, has emerged as a promising technique to improve optimization and generalization. Recently, a completely novel learning algorithm, called forward-forward, has been introduced. The forward-forward algorithm has shown great potential due to its simplicity. However, its application has been limited to smaller datasets due to scalability issues. To this end, this thesis addresses the scalability limitations of the forward-forward algorithm by proposing a novel improvement termed the Trifecta.",
    "start": "Sep 2022",
    "end": "Jun 2023",
    "image": "static/ff.png",
    "paper": "https://arxiv.org/abs/2311.18130",
    "presentation": "static/thesis_ppt.pdf"
  }
]